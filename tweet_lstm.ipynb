{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Sentiment Analysis Solution\n",
    "\n",
    "\n",
    "Here is a Python Keras implementation of the Long-Short-Term-Memory Neural net to identify tweets referring to natural and human-made disasters from more mundane tweets. \n",
    "\n",
    "LSTMS have the advantage over Naive Bayes classifiers in that they are able to contextualise a word based on where it appears in a sequence (e.g. 'Not Bad' can be interpreted differently to 'Bad'). They offer an advancement over recurrent neural nets in that while both have memory of previous words in the sequence, LSTM's have a longer term memory and can pull context from many words ago. They are often referred to as 'fancy' RNN's.\n",
    "\n",
    "The code below test the LSTM on the publicly available dataset prior to implementation for more specific tech-related pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import pandas as pd\n",
    "import keras\n",
    "import numpy as np\n",
    "import utils\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the train data and split into train test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train.csv')\n",
    "X = list(df['text'])\n",
    "y = list(df['target'])\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize and padd the input X_train data to uniform size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = keras.preprocessing.text.Tokenizer(num_words=1000)\n",
    "tok.fit_on_texts(X_train_raw)\n",
    "#summarise top words\n",
    "word_counts = pd.DataFrame(dict(tok.word_counts),index=['count']).transpose().sort_values(by='count',ascending=False)\n",
    "num_words = len(word_counts)\n",
    "tok_dict = dict(tok.index_word)\n",
    "print(str(num_words)+' distinct words found')\n",
    "print('top 10...')\n",
    "print(word_counts.head(10))\n",
    "print('bottom 10...')\n",
    "print(word_counts.tail(10))\n",
    "# summarize what was learned\n",
    "#print(tok.word_counts)\n",
    "#print(tok.document_count)\n",
    "#print(tok.word_index)\n",
    "#print(tok.word_docs)\n",
    "# integer encode documents\n",
    "X_train = tok.texts_to_sequences(X_train_raw)\n",
    "#padd so all same length\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train,padding='post')\n",
    "\n",
    "#transform test data\n",
    "X_test = tok.texts_to_sequences(X_test_raw)\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test,padding='post',maxlen=X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_dict = utils.load_embeddings('./data/non_tracked/glove.6B.100d.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the embedding_matrix for the words in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_words = list(word_embeddings_dict.keys())\n",
    "wordvec_dim = word_embeddings_dict[embeddings_words[0]].shape[0]\n",
    "embedding_matrix = np.zeros((num_words, wordvec_dim))\n",
    "for i, word in tok_dict.items():\n",
    "    # Look up the word embedding\n",
    "    vector = word_embeddings_dict.get(word, None)\n",
    "    # Record in matrix\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i, :] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = keras.Sequential()\n",
    "# initialise Ebedding layer num_words = len(idx_word) + 1 to deal with 0 padding\n",
    "# input_length is the number of words ids per sample e.g 28\n",
    "# NOT the sample size of the training data\n",
    "# you do not need to supply that info\n",
    "model_lstm.add(keras.layers.Embedding(input_dim=num_words,\n",
    "                                      input_length=X_train.shape[1],\n",
    "                                      output_dim=wordvec_dim,\n",
    "                                      weights=[embedding_matrix],\n",
    "                                      trainable=False,\n",
    "                                      mask_zero=True))\n",
    "\n",
    "# words which are not in the pretrained embeddings (with value 0) are ignored\n",
    "model_lstm.add(keras.layers.Masking(mask_value=0.0))\n",
    "\n",
    "# Recurrent layer\n",
    "model_lstm.add(keras.layers.LSTM(200, return_sequences=False))\n",
    "model_lstm.add(keras.layers.Dropout(0.4))\n",
    "# model_lstm.add(keras.layers.LSTM(28, return_sequences=True))\n",
    "# model_lstm.add(keras.layers.Dropout(0.2))\n",
    "# model_lstm.add(keras.layers.LSTM(28, return_sequences=True))\n",
    "# model_lstm.add(keras.layers.Dropout(0.2))\n",
    "# model_lstm.add(keras.layers.LSTM(28, return_sequences=False))\n",
    "\n",
    "# Output layer\n",
    "model_lstm.add(keras.layers.Dense(1))\n",
    "model_lstm.add(keras.layers.Activation('sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model_lstm.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# model summary\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picklefile = './models/lstm.pickle'\n",
    "os.system('rm ' + picklefile)\n",
    "pickle_out = open(picklefile, \"wb\")\n",
    "kwargs = {'epochs':10,'batch_size':128}\n",
    "pickle.dump({'model':model_lstm,'model_name':'LSTM','X_train':X_train,'y_train':y_train,\n",
    "             'X_test':X_test,'y_test':y_test,'kwargs':kwargs}, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Fit here or reserve for benchmarking script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_fit = False\n",
    "if perform_fit:\n",
    "    model_lstm.fit(np.array(X_train), np.array(y_train), **kwargs)\n",
    "    y_pred = model_lstm.predict(np.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional run K-fold cross validation to asses model performance\n",
    "#from benchmarking_models import run_cv\n",
    "#y_pred = run_cv(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
