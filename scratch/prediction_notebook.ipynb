{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0: Library and function import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Remove the useless url tag \n",
    "def remove_url(raw_str):\n",
    "    clean_str = re.sub(r'http\\S+', '', raw_str)\n",
    "    return clean_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1: Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "test_df = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomization\n",
    "state = 1\n",
    "train_df = train_df.sample(frac=1,random_state=state)\n",
    "test_df = test_df.sample(frac=1,random_state=state)\n",
    "train_df.reset_index(inplace=True, drop=True) \n",
    "test_df.reset_index(inplace=True, drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** 2: Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit: Gunes Evitan\n",
    "#https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-full-cleaning#4.-Embeddings-&-Text-Cleaning\n",
    "def clean(tweet):\n",
    "    \n",
    "    # Punctuations at the start or end of words    \n",
    "    #for punctuation in \"#@!?()[]*%\":\n",
    "    #    tweet = tweet.replace(punctuation, f' {punctuation} ').strip()\n",
    "        \n",
    "    #tweet = tweet.replace('...', ' ... ').strip()\n",
    "    #tweet = tweet.replace(\"'\", \" ' \").strip()        \n",
    "    \n",
    "    # Special characters\n",
    "    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n",
    "    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
    "    tweet = re.sub(r\"å_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n",
    "    tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"åÈ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n",
    "    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n",
    "    tweet = re.sub(r\"å¨\", \"\", tweet)\n",
    "    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n",
    "    \n",
    "    # Contractions\n",
    "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
    "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
    "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
    "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
    "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
    "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
    "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
    "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
    "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
    "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
    "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
    "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
    "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
    "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
    "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
    "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
    "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
    "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
    "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
    "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
    "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
    "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
    "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
    "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
    "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
    "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
    "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
    "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
    "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
    "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
    "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
    "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
    "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
    "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
    "    \n",
    "    # Character entity references\n",
    "    tweet = re.sub(r\"&gt;\", \">\", tweet)\n",
    "    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n",
    "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n",
    "        \n",
    "    # Typos, slang and informal abbreviations\n",
    "    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n",
    "    tweet = re.sub(r\"w/\", \"with\", tweet)\n",
    "    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n",
    "    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n",
    "    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n",
    "    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n",
    "    tweet = re.sub(r\"<3\", \"love\", tweet)\n",
    "    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n",
    "    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n",
    "    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\n",
    "    tweet = re.sub(r\"chest/torso\", \"chest / torso\", tweet)\n",
    "    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n",
    "    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\n",
    "    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n",
    "    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n",
    "    \n",
    "    # Separating other punctuations\n",
    "    tweet = re.sub(r\"MH370:\", \"MH370 :\", tweet)\n",
    "    tweet = re.sub(r\"PM:\", \"Prime Minister :\", tweet)\n",
    "    tweet = re.sub(r\"Legionnaires:\", \"Legionnaires :\", tweet)\n",
    "    tweet = re.sub(r\"Latest:\", \"Latest :\", tweet)\n",
    "    tweet = re.sub(r\"Crash:\", \"Crash :\", tweet)\n",
    "    tweet = re.sub(r\"News:\", \"News :\", tweet)\n",
    "    tweet = re.sub(r\"derailment:\", \"derailment :\", tweet)\n",
    "    tweet = re.sub(r\"attack:\", \"attack :\", tweet)\n",
    "    tweet = re.sub(r\"Saipan:\", \"Saipan :\", tweet)\n",
    "    tweet = re.sub(r\"Photo:\", \"Photo :\", tweet)\n",
    "    tweet = re.sub(r\"Funtenna:\", \"Funtenna :\", tweet)\n",
    "    tweet = re.sub(r\"quiz:\", \"quiz :\", tweet)\n",
    "    tweet = re.sub(r\"VIDEO:\", \"VIDEO :\", tweet)\n",
    "    tweet = re.sub(r\"MP:\", \"MP :\", tweet)\n",
    "    tweet = re.sub(r\"UTC2015-08-05\", \"UTC 2015-08-05\", tweet)\n",
    "    tweet = re.sub(r\"California:\", \"California :\", tweet)\n",
    "    tweet = re.sub(r\"horror:\", \"horror :\", tweet)\n",
    "    tweet = re.sub(r\"Past:\", \"Past :\", tweet)\n",
    "    tweet = re.sub(r\"Time2015-08-06\", \"Time 2015-08-06\", tweet)\n",
    "    tweet = re.sub(r\"here:\", \"here :\", tweet)\n",
    "    tweet = re.sub(r\"fires.\", \"fires .\", tweet)\n",
    "    tweet = re.sub(r\"Forest:\", \"Forest :\", tweet)\n",
    "    tweet = re.sub(r\"Cramer:\", \"Cramer :\", tweet)\n",
    "    tweet = re.sub(r\"Chile:\", \"Chile :\", tweet)\n",
    "    tweet = re.sub(r\"link:\", \"link :\", tweet)\n",
    "    tweet = re.sub(r\"crash:\", \"crash :\", tweet)\n",
    "    tweet = re.sub(r\"Video:\", \"Video :\", tweet)\n",
    "    tweet = re.sub(r\"Bestnaijamade:\", \"bestnaijamade :\", tweet)\n",
    "    tweet = re.sub(r\"NWS:\", \"National Weather Service :\", tweet)\n",
    "    tweet = re.sub(r\".caught\", \". caught\", tweet)\n",
    "    tweet = re.sub(r\"Hobbit:\", \"Hobbit :\", tweet)\n",
    "    tweet = re.sub(r\"2015:\", \"2015 :\", tweet)\n",
    "    tweet = re.sub(r\"post:\", \"post :\", tweet)\n",
    "    tweet = re.sub(r\"BREAKING:\", \"BREAKING :\", tweet)\n",
    "    tweet = re.sub(r\"Island:\", \"Island :\", tweet)\n",
    "    tweet = re.sub(r\"Med:\", \"Med :\", tweet)\n",
    "    tweet = re.sub(r\"97/Georgia\", \"97 / Georgia\", tweet)\n",
    "    tweet = re.sub(r\"Here:\", \"Here :\", tweet)\n",
    "    tweet = re.sub(r\"horror;\", \"horror ;\", tweet)\n",
    "    tweet = re.sub(r\"people;\", \"people ;\", tweet)\n",
    "    tweet = re.sub(r\"refugees;\", \"refugees ;\", tweet)\n",
    "    tweet = re.sub(r\"Genocide;\", \"Genocide ;\", tweet)\n",
    "    tweet = re.sub(r\".POTUS\", \". POTUS\", tweet)\n",
    "    tweet = re.sub(r\"Collision-No\", \"Collision - No\", tweet)\n",
    "    tweet = re.sub(r\"Rear-\", \"Rear -\", tweet)\n",
    "    tweet = re.sub(r\"Broadway:\", \"Broadway :\", tweet)\n",
    "    tweet = re.sub(r\"Correction:\", \"Correction :\", tweet)\n",
    "    tweet = re.sub(r\"UPDATE:\", \"UPDATE :\", tweet)\n",
    "    tweet = re.sub(r\"Times:\", \"Times :\", tweet)\n",
    "    tweet = re.sub(r\"RT:\", \"RT :\", tweet)\n",
    "    tweet = re.sub(r\"Police:\", \"Police :\", tweet)\n",
    "    tweet = re.sub(r\"Training:\", \"Training :\", tweet)\n",
    "    tweet = re.sub(r\"Hawaii:\", \"Hawaii :\", tweet)\n",
    "    tweet = re.sub(r\"Selfies:\", \"Selfies :\", tweet)\n",
    "    tweet = re.sub(r\"Content:\", \"Content :\", tweet)\n",
    "    tweet = re.sub(r\"101:\", \"101 :\", tweet)\n",
    "    tweet = re.sub(r\"story:\", \"story :\", tweet)\n",
    "    tweet = re.sub(r\"injured:\", \"injured :\", tweet)\n",
    "    tweet = re.sub(r\"poll:\", \"poll :\", tweet)\n",
    "    tweet = re.sub(r\"Guide:\", \"Guide :\", tweet)\n",
    "    tweet = re.sub(r\"Update:\", \"Update :\", tweet)\n",
    "    tweet = re.sub(r\"alarm:\", \"alarm :\", tweet)\n",
    "    tweet = re.sub(r\"floods:\", \"floods :\", tweet)\n",
    "    tweet = re.sub(r\"Flood:\", \"Flood :\", tweet)\n",
    "    tweet = re.sub(r\"MH370;\", \"MH370 ;\", tweet)\n",
    "    tweet = re.sub(r\"life:\", \"life :\", tweet)\n",
    "    tweet = re.sub(r\"crush:\", \"crush :\", tweet)\n",
    "    tweet = re.sub(r\"now:\", \"now :\", tweet)\n",
    "    tweet = re.sub(r\"Vote:\", \"Vote :\", tweet)\n",
    "    tweet = re.sub(r\"Catastrophe.\", \"Catastrophe .\", tweet)\n",
    "    tweet = re.sub(r\"library:\", \"library :\", tweet)\n",
    "    tweet = re.sub(r\"Bush:\", \"Bush :\", tweet)\n",
    "    tweet = re.sub(r\";ACCIDENT\", \"; ACCIDENT\", tweet)\n",
    "    tweet = re.sub(r\"accident:\", \"accident :\", tweet)\n",
    "    tweet = re.sub(r\"Taiwan;\", \"Taiwan ;\", tweet)\n",
    "    tweet = re.sub(r\"Map:\", \"Map :\", tweet)\n",
    "    tweet = re.sub(r\"failure:\", \"failure :\", tweet)\n",
    "    tweet = re.sub(r\"150-Foot\", \"150 - Foot\", tweet)\n",
    "    tweet = re.sub(r\"failure:\", \"failure :\", tweet)\n",
    "    tweet = re.sub(r\"prefer:\", \"prefer :\", tweet)\n",
    "    tweet = re.sub(r\"CNN:\", \"CNN :\", tweet)\n",
    "    tweet = re.sub(r\"Oops:\", \"Oops :\", tweet)\n",
    "    tweet = re.sub(r\"Disco:\", \"Disco :\", tweet)\n",
    "    tweet = re.sub(r\"Disease:\", \"Disease :\", tweet)\n",
    "    tweet = re.sub(r\"Grows:\", \"Grows :\", tweet)\n",
    "    tweet = re.sub(r\"projected:\", \"projected :\", tweet)\n",
    "    tweet = re.sub(r\"Pakistan.\", \"Pakistan .\", tweet)\n",
    "    tweet = re.sub(r\"ministers:\", \"ministers :\", tweet)\n",
    "    tweet = re.sub(r\"Photos:\", \"Photos :\", tweet)\n",
    "    tweet = re.sub(r\"Disease:\", \"Disease :\", tweet)\n",
    "    tweet = re.sub(r\"pres:\", \"press :\", tweet)\n",
    "    tweet = re.sub(r\"winds.\", \"winds .\", tweet)\n",
    "    tweet = re.sub(r\"MPH.\", \"MPH .\", tweet)\n",
    "    tweet = re.sub(r\"PHOTOS:\", \"PHOTOS :\", tweet)\n",
    "    tweet = re.sub(r\"Time2015-08-05\", \"Time 2015-08-05\", tweet)\n",
    "    tweet = re.sub(r\"Denmark:\", \"Denmark :\", tweet)\n",
    "    tweet = re.sub(r\"Articles:\", \"Articles :\", tweet)\n",
    "    tweet = re.sub(r\"Crash:\", \"Crash :\", tweet)\n",
    "    tweet = re.sub(r\"casualties.:\", \"casualties .:\", tweet)\n",
    "    tweet = re.sub(r\"Afghanistan:\", \"Afghanistan :\", tweet)\n",
    "    tweet = re.sub(r\"Day:\", \"Day :\", tweet)\n",
    "    tweet = re.sub(r\"AVERTED:\", \"AVERTED :\", tweet)\n",
    "    tweet = re.sub(r\"sitting:\", \"sitting :\", tweet)\n",
    "    tweet = re.sub(r\"Multiplayer:\", \"Multiplayer :\", tweet)\n",
    "    tweet = re.sub(r\"Kaduna:\", \"Kaduna :\", tweet)\n",
    "    tweet = re.sub(r\"favorite:\", \"favorite :\", tweet)\n",
    "    tweet = re.sub(r\"home:\", \"home :\", tweet)\n",
    "    tweet = re.sub(r\"just:\", \"just :\", tweet)\n",
    "    tweet = re.sub(r\"Collision-1141\", \"Collision - 1141\", tweet)\n",
    "    tweet = re.sub(r\"County:\", \"County :\", tweet)\n",
    "    tweet = re.sub(r\"Duty:\", \"Duty :\", tweet)\n",
    "    tweet = re.sub(r\"page:\", \"page :\", tweet)\n",
    "    tweet = re.sub(r\"Attack:\", \"Attack :\", tweet)\n",
    "    tweet = re.sub(r\"Minecraft:\", \"Minecraft :\", tweet)\n",
    "    tweet = re.sub(r\"wounds;\", \"wounds ;\", tweet)\n",
    "    tweet = re.sub(r\"Shots:\", \"Shots :\", tweet)\n",
    "    tweet = re.sub(r\"shots:\", \"shots :\", tweet)\n",
    "    tweet = re.sub(r\"Gunfire:\", \"Gunfire :\", tweet)\n",
    "    tweet = re.sub(r\"hike:\", \"hike :\", tweet)\n",
    "    tweet = re.sub(r\"Email:\", \"Email :\", tweet)\n",
    "    tweet = re.sub(r\"System:\", \"System :\", tweet)\n",
    "    tweet = re.sub(r\"Radio:\", \"Radio :\", tweet)\n",
    "    tweet = re.sub(r\"King:\", \"King :\", tweet)\n",
    "    tweet = re.sub(r\"upheaval:\", \"upheaval :\", tweet)\n",
    "    tweet = re.sub(r\"tragedy;\", \"tragedy ;\", tweet)\n",
    "    tweet = re.sub(r\"HERE:\", \"HERE :\", tweet)\n",
    "    tweet = re.sub(r\"terrorism:\", \"terrorism :\", tweet)\n",
    "    tweet = re.sub(r\"police:\", \"police :\", tweet)\n",
    "    tweet = re.sub(r\"Mosque:\", \"Mosque :\", tweet)\n",
    "    tweet = re.sub(r\"Rightways:\", \"Rightways :\", tweet)\n",
    "    tweet = re.sub(r\"Brooklyn:\", \"Brooklyn :\", tweet)\n",
    "    tweet = re.sub(r\"Arrived:\", \"Arrived :\", tweet)\n",
    "    tweet = re.sub(r\"Home:\", \"Home :\", tweet)\n",
    "    tweet = re.sub(r\"Earth:\", \"Earth :\", tweet)\n",
    "    tweet = re.sub(r\"three:\", \"three :\", tweet)\n",
    "    \n",
    "    # Hashtags and usernames\n",
    "    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n",
    "    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n",
    "    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n",
    "    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n",
    "    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n",
    "    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n",
    "    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n",
    "    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n",
    "    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n",
    "    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n",
    "    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n",
    "    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n",
    "    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n",
    "    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n",
    "    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n",
    "    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n",
    "    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n",
    "    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n",
    "    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n",
    "    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n",
    "    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n",
    "    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n",
    "    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n",
    "    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n",
    "    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n",
    "    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n",
    "    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n",
    "    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\n",
    "    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n",
    "    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n",
    "    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n",
    "    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n",
    "    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n",
    "    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n",
    "    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n",
    "    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n",
    "    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n",
    "    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n",
    "    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\n",
    "    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n",
    "    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n",
    "    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n",
    "    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n",
    "    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n",
    "    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n",
    "    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n",
    "    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n",
    "    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n",
    "    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n",
    "    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n",
    "    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n",
    "    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n",
    "    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n",
    "    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\n",
    "    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n",
    "    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\n",
    "    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\n",
    "    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n",
    "    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\n",
    "    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n",
    "    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n",
    "    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n",
    "    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor2(text):\n",
    "    text = text.replace('%20',' ')\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n",
    "    text = (re.sub('[^a-zA-Z0-9_]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text'].apply(lambda x:remove_url(x))\n",
    "train_df['keyword_cleaned'] = train_df['keyword'].copy().apply(lambda x : clean(str(x))).apply(lambda x : preprocessor2(x))\n",
    "train_df['location_cleaned'] = train_df['location'].copy().apply(lambda x : clean(str(x))).apply(lambda x : preprocessor2(x))\n",
    "train_df['text_cleaned'] = train_df['text'].copy().apply(lambda x : clean(x)).apply(lambda x : preprocessor2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['text'] = test_df['text'].apply(lambda x:remove_url(x))\n",
    "test_df['keyword_cleaned'] = test_df['keyword'].copy().apply(lambda x : clean(str(x))).apply(lambda x : preprocessor2(x))\n",
    "test_df['location_cleaned'] = test_df['location'].copy().apply(lambda x : clean(str(x))).apply(lambda x : preprocessor2(x))\n",
    "test_df['text_cleaned'] = test_df['text'].copy().apply(lambda x : clean(x)).apply(lambda x : preprocessor2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding of the training dataset - basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target count\n",
    "fig, ax = plt.subplots(figsize = (8,5))\n",
    "pd.value_counts(train_df['target']).plot(kind=\"bar\")\n",
    "ax.set_title('Target Count')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 20 Locations with most target occurance\n",
    "loc_pos = train_df[(train_df.location_cleaned != 'nan') & (train_df.location_cleaned != ' ') & (train_df['target'] == 1)]['location_cleaned'].value_counts()\n",
    "loc_neg = train_df[(train_df.location_cleaned != 'nan') & (train_df.location_cleaned != ' ') & (train_df['target'] == 0)]['location_cleaned'].value_counts()\n",
    "\n",
    "loc_pos_dict = loc_pos[:20].to_dict()\n",
    "loc_neg_dict = loc_neg[:20].to_dict()\n",
    "\n",
    "names0 = list(loc_neg_dict.keys())\n",
    "values0 = list(loc_neg_dict.values())\n",
    "names1 = list(loc_pos_dict.keys())\n",
    "values1 = list(loc_pos_dict.values())\n",
    "\n",
    "#Graph\n",
    "fig, (ax1, ax2) = plt.subplots(figsize = (20,5), nrows=1, ncols=2)\n",
    "\n",
    "ax1.bar(range(len(loc_pos_dict)),values1,tick_label=names1)\n",
    "ax1.set_xticklabels(names1, rotation=\"vertical\")\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.grid(True)\n",
    "ax1.set_title('Location with most Pos target')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "ax2.bar(range(len(loc_neg_dict)),values0,tick_label=names0)\n",
    "ax2.set_xticklabels(names0, rotation=\"vertical\")\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.grid(True)\n",
    "ax2.set_title('Location with most Neg target')\n",
    "ax2.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we can see some same meaning words using different abbreviation, so that we try to make a function to align these words\n",
    "def preprocessor3(text):\n",
    "    text = re.sub(r'^washington d c ', \"washington dc\", text)\n",
    "    text = re.sub(r'^washington +[\\w]*', \"washington dc\", text)\n",
    "    text = re.sub(r'^new york +[\\w]*', \"new york\", text)\n",
    "    text = re.sub(r'^nyc$', \"new york\", text)\n",
    "    text = re.sub(r'^chicago +[\\w]*', \"chicago\", text)\n",
    "    text = re.sub(r'^california +[\\w]*', \"california\", text)\n",
    "    text = re.sub(r'^los angeles +[\\w]*', \"los angeles\", text)\n",
    "    text = re.sub(r'^san francisco +[\\w]*', \"san francisco\", text)\n",
    "    text = re.sub(r'^london +[\\w]*', \"london\", text)\n",
    "    text = re.sub(r'^usa$', \"united states\", text)\n",
    "    text = re.sub(r'^us$', \"united states\", text)\n",
    "    text = re.sub(r'^uk$', \"united kingdom\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocessor4(text):\n",
    "    abb = ['ak', 'al', 'az', 'ar', 'ca', 'co',\n",
    "           'ct', 'de', 'dc', 'fl', 'ga', 'hi',\n",
    "           'id', 'il', 'in', 'ia', 'ks', 'ky',\n",
    "           'la', 'me', 'mt', 'ne', 'nv', 'nh',\n",
    "           'nj', 'nm', 'ny', 'nc', 'nd', 'oh',\n",
    "           'ok', 'or', 'md', 'ma', 'mi', 'mn',\n",
    "           'ms', 'mo', 'pa', 'ri', 'sc', 'sd',\n",
    "           'tn', 'tx', 'ut', 'vt', 'va', 'wa',\n",
    "           'wv', 'wi', 'wy']\n",
    "    \n",
    "    for i in abb:\n",
    "        text = re.sub(r' {0}$'.format(i), '', text)\n",
    "        \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['location_cleaned'] = train_df['location_cleaned'].copy().apply(lambda x : preprocessor3(x)).apply(lambda x : preprocessor4(x))\n",
    "train_df['text_cleaned'] = train_df['text_cleaned'].copy().apply(lambda x : preprocessor3(x)).apply(lambda x : preprocessor4(x))\n",
    "\n",
    "test_df['location_cleaned'] = test_df['location_cleaned'].copy().apply(lambda x : preprocessor3(x)).apply(lambda x : preprocessor4(x))\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].copy().apply(lambda x : preprocessor3(x)).apply(lambda x : preprocessor4(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding of the training dataset - word cloud visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_pos_dict = loc_pos[:].to_dict()\n",
    "loc_neg_dict = loc_neg[:].to_dict()\n",
    "\n",
    "loc_list = list(loc_pos_dict.keys()) + list(loc_neg_dict.keys())\n",
    "unique_loc = []\n",
    "lower_loc = []\n",
    "for x in loc_list:\n",
    "    if x not in unique_loc:\n",
    "        unique_loc.append(x)\n",
    "        \n",
    "for x in unique_loc:\n",
    "    lower_loc.append(x.lower().replace(',','').replace('.',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "stopwords = list(STOPWORDS)+['will','may','one','now','nan','don'] #+ lower_loc\n",
    "\n",
    "class wc_base2:\n",
    "    def __init__(self, data):\n",
    "        self.temp = data.apply(lambda x: ' '.join([word for word in x.split()]))\n",
    "        self.text = \" \".join(word for word in data)\n",
    "        self.wordlist = []\n",
    "        \n",
    "    def plot_wc(self, mask=None, max_words=200, figure_size=(20,10), title=None, stopwords=stopwords):\n",
    "\n",
    "        print (\"There are {} words in the combination of all review.\".format(len(self.text)))\n",
    "\n",
    "        wordcloud = WordCloud(background_color='black',\n",
    "                        stopwords=stopwords,\n",
    "                        max_words = max_words,\n",
    "                        collocations=False,\n",
    "                        random_state = 10,\n",
    "                        width = 800,\n",
    "                        height =400)\n",
    "\n",
    "        wordcloud.generate(self.text)\n",
    "         \n",
    "        self.wordlist = list(wordcloud.words_.keys())\n",
    "\n",
    "        plt.figure(figsize=figure_size)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.title(title)\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = wc_base2(train_df[train_df.target == 0].text_cleaned)\n",
    "wc.plot_wc(title=\"Word Cloud of tweets with Negative target\")\n",
    "wc_s = set(wc.wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc2 = wc_base2(train_df[train_df.target == 1].text_cleaned)\n",
    "wc2.plot_wc(title=\"Word Cloud of tweets with Positive target\")\n",
    "wc2_s = set(wc2.wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text_cleaned'] = train_df['text_cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "test_df['text_cleaned'] = test_df['text_cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Data Augmentation -- EDA (Random  Swap - RS) and EDA (Random  Deletion - RD) - for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['index_no'] = train_df.index\n",
    "train_df['sent_w_index'] = train_df['text_cleaned'] + ' ' + train_df['index_no'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_swap(text):\n",
    "    text_list = text.split()\n",
    "    seed = int(text_list[-1])\n",
    "\n",
    "    text_list=text_list[:-1]\n",
    "    text_length = len(text_list)\n",
    "   \n",
    "    np.random.seed(seed)\n",
    "    a = np.random.randint(0, text_length,size=2)\n",
    "    #print(a)\n",
    "\n",
    "    temp_a = text_list[a[0]]\n",
    "    temp_b = text_list[a[1]]\n",
    "    \n",
    "    text_list[a[0]] = temp_b\n",
    "    text_list[a[1]] = temp_a\n",
    "    \n",
    "    redo = ' '.join([str(i) for i in text_list])\n",
    "   \n",
    "    return redo    \n",
    "\n",
    "def random_del(text):\n",
    "    text_list = text.split()\n",
    "    seed = int(text_list[-1])\n",
    "\n",
    "    text_list=text_list[:-1]\n",
    "    text_length = len(text_list)\n",
    "   \n",
    "    np.random.seed(seed)\n",
    "    a = np.random.randint(0, text_length,size=1)\n",
    "    text_list.pop((a[0]))\n",
    "    \n",
    "    redo = ' '.join([str(i) for i in text_list])\n",
    "      \n",
    "    return redo    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['da_text_cleaned'] = train_df['sent_w_index'].apply(lambda x:random_swap(x))\n",
    "train_df['da_text_cleaned2'] = train_df['sent_w_index'].apply(lambda x:random_del(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(['index_no','sent_w_index'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df1 = list(zip(train_df.target,train_df.keyword_cleaned,train_df.location_cleaned,train_df.da_text_cleaned))\n",
    "temp_df2 = list(zip(train_df.target,train_df.keyword_cleaned,train_df.location_cleaned,train_df.da_text_cleaned2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame(temp_df1, columns =['target','keyword_cleaned','location_cleaned','text_cleaned'])\n",
    "y = pd.DataFrame(temp_df2, columns =['target','keyword_cleaned','location_cleaned','text_cleaned'])\n",
    "\n",
    "train_df.drop(['da_text_cleaned','da_text_cleaned2'], axis=1,inplace=True)\n",
    "\n",
    "z = pd.concat([train_df,x,y], axis=0, join='outer', ignore_index=False, keys=None, sort = False)\n",
    "z = z[['id','keyword_cleaned','location_cleaned','text_cleaned','target']].copy()\n",
    "z.reset_index(inplace=True, drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = z \n",
    "test_df = test_df[['id','keyword_cleaned','location_cleaned','text_cleaned']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomization\n",
    "state = 1\n",
    "train_df = train_df.sample(frac=1,random_state=state)\n",
    "train_df.reset_index(inplace=True, drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Build a Simple deep learning model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "top_word = 35000\n",
    "\n",
    "text_lengths = [len(x.split()) for x in (train_df.text_cleaned)]\n",
    "#text_lengths = [x for x in text_lengths if x < 50]\n",
    "plt.hist(text_lengths, bins=25)\n",
    "plt.title('Histogram of # of Words in Texts')\n",
    "\n",
    "tok = Tokenizer(num_words=top_word)\n",
    "tok.fit_on_texts((train_df['text_cleaned']+train_df['keyword_cleaned']+train_df['location_cleaned']))\n",
    "\n",
    "max_words = max(text_lengths) + 1\n",
    "max_words_ky = max([len(x.split()) for x in (train_df.keyword_cleaned)]) + 1\n",
    "max_words_lc = max([len(x.split()) for x in (train_df.location_cleaned)]) + 1\n",
    "print(\"top_word: \", str(top_word))\n",
    "print(\"max_words: \", str(max_words))\n",
    "print(\"max_words_ky: \", str(max_words_ky))\n",
    "print(\"max_words_lc: \", str(max_words_lc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training set\n",
    "val_value = 5000\n",
    "\n",
    "X_train_tx = tok.texts_to_sequences(train_df['text_cleaned'])\n",
    "X_train_ky = tok.texts_to_sequences(train_df['keyword_cleaned'])\n",
    "X_train_lc = tok.texts_to_sequences(train_df['location_cleaned'])\n",
    "\n",
    "X_test_tx = tok.texts_to_sequences(test_df['text_cleaned'])\n",
    "X_test_ky = tok.texts_to_sequences(test_df['keyword_cleaned'])\n",
    "X_test_lc = tok.texts_to_sequences(test_df['location_cleaned'])\n",
    "\n",
    "\n",
    "Y_train = train_df['target']\n",
    "\n",
    "print('Found %s unique tokens.' % len(tok.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "# One-hot category\n",
    "Y_train = to_categorical(Y_train)\n",
    "print(\"Y_train.shape: \", Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tx = sequence.pad_sequences(X_train_tx, maxlen=max_words)\n",
    "X_train_ky = sequence.pad_sequences(X_train_ky, maxlen=max_words_ky)\n",
    "X_train_lc = sequence.pad_sequences(X_train_lc, maxlen=max_words_lc)\n",
    "\n",
    "X_test_tx = sequence.pad_sequences(X_test_tx, maxlen=max_words)\n",
    "X_test_ky = sequence.pad_sequences(X_test_ky, maxlen=max_words_ky)\n",
    "X_test_lc = sequence.pad_sequences(X_test_lc, maxlen=max_words_lc)\n",
    "\n",
    "print(\"X_train_tx.shape: \", X_train_tx.shape)\n",
    "print(\"X_train_ky.shape: \", X_train_ky.shape)\n",
    "print(\"X_train_lc.shape: \", X_train_lc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('/kaggle/input/glove6b100dtxt/glove.6B.100d.txt', encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = zeros((top_word, embedding_dim))\n",
    "for word, index in tok.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Input, Conv1D, MaxPooling1D\n",
    "from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = Input(shape=(max_words,))\n",
    "embedding_layer1 = Embedding(top_word, 100, weights=[embedding_matrix], input_length=max_words, trainable=False)(input1)\n",
    "dropout1 = Dropout(0.2)(embedding_layer1)\n",
    "lstm1_1 = LSTM(128,return_sequences = True)(dropout1)\n",
    "lstm1_2 = LSTM(128,return_sequences = True)(lstm1_1)\n",
    "lstm1_2a = LSTM(128,return_sequences = True)(lstm1_2)\n",
    "lstm1_3 = LSTM(128)(lstm1_2a)\n",
    "\n",
    "input2 = Input(shape=(max_words_ky,))\n",
    "embedding_layer2 = Embedding(top_word, 100, weights=[embedding_matrix], input_length=max_words_ky, trainable=False)(input2)\n",
    "dropout2 = Dropout(0.2)(embedding_layer2)\n",
    "lstm2_1 = LSTM(64,return_sequences = True)(dropout2)\n",
    "lstm2_2 = LSTM(64,return_sequences = True)(lstm2_1)\n",
    "lstm2_3 = LSTM(64)(lstm2_2)\n",
    "\n",
    "input3 = Input(shape=(max_words_lc,))\n",
    "embedding_layer3 = Embedding(top_word, 100, weights=[embedding_matrix], input_length=max_words_lc, trainable=False)(input3)\n",
    "dropout3 = Dropout(0.2)(embedding_layer3)\n",
    "lstm3_1 = LSTM(32,return_sequences = True)(dropout3)\n",
    "lstm3_2 = LSTM(32,return_sequences = True)(lstm3_1)\n",
    "lstm3_3 = LSTM(32)(lstm3_2)\n",
    "\n",
    "merge = concatenate([lstm1_3, lstm2_3,lstm3_3])\n",
    "\n",
    "dropout = Dropout(0.8)(merge)\n",
    "dense1 = Dense(256, activation='relu')(dropout)\n",
    "dense2 = Dense(128, activation='relu')(dense1)\n",
    "output = Dense(2, activation='softmax')(dense2)\n",
    "model = Model(inputs=[input1,input2,input3], outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min',verbose=1, patience = 3)\n",
    "history = model.fit([X_train_tx,X_train_ky,X_train_lc], Y_train, validation_split=0.2, epochs=30, batch_size=64, verbose=2, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_eva (loss,val_loss,acc,val_acc):\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    \n",
    "    epochs = range(1,len(loss)+1)\n",
    "    plt.plot(epochs, loss,'b-o', label ='Training Loss')\n",
    "    plt.plot(epochs, val_loss,'r-o', label ='Validation Loss')\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    epochs = range(1, len(acc)+1)\n",
    "    plt.plot(epochs, acc, \"b-o\", label=\"Training Acc\")\n",
    "    plt.plot(epochs, val_acc, \"r-o\", label=\"Validation Acc\")\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_eva(history.history['loss'], history.history['val_loss'], history.history['accuracy'], history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('nlp_disaster.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = Model()\n",
    "model = load_model('nlp_disaster.h5')\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict([X_test_tx,X_test_ky,X_test_lc], batch_size=32, verbose=2)\n",
    "Y_pred = np.argmax(Y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(Y_pred, columns=['target'])\n",
    "result = pd.concat([test_df,pred_df], axis=1, join='outer', ignore_index=False, keys=None, sort = False)\n",
    "result = result[['id','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('sample_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
